{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGram.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-i-Z-7axjPU",
        "colab_type": "text"
      },
      "source": [
        "## N-gram model to form a probability distribution of words occuring within the different categories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXs7SwVzxb_n",
        "colab_type": "code",
        "outputId": "b9753273-1da1-4f6b-f620-18ccccd9d1b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk \n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W58P1yTrxuZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def clean_data(df):\n",
        "    # Get rid of documents where the text is null\n",
        "    indices_notNull = df.index[pd.notnull(df['document_text'])]\n",
        "    df = df.loc[indices_notNull]  # Pandas return 1 indexing, so use loc instead of iloc\n",
        "\n",
        "    # Delete rows with no incident date\n",
        "    indices_notNull = df.index[pd.notnull(df['incident_date'])]\n",
        "    df = df.loc[indices_notNull]\n",
        "\n",
        "    # Delete rows with null entries in categories\n",
        "\n",
        "    # Divide document into two sections, before 2015 and 2015 onwards\n",
        "    df['incident_date'] = pd.to_datetime(df['incident_date'])\n",
        "    df['incident_date'] = pd.DatetimeIndex(df['incident_date']).year\n",
        "    df_b2015 = df[df['incident_date'] < 2015]\n",
        "    df_a2015 = df[df['incident_date'] >= 2015]\n",
        "\n",
        "    return df_b2015, df_a2015\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SGZdJhQx6__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def divide_by_category(df):\n",
        "    '''\n",
        "    This function will divide the data into 11 categories by storing their indices\n",
        "    :param df: The document tags\n",
        "    :return:\n",
        "    '''\n",
        "    columns_of_interest = [\n",
        "        'nudity_penetration', 'sexual_harassment_remarks',\n",
        "       'sexual_humiliation_extortion_or_sex_work', 'tasers', 'trespass',\n",
        "       'racial_slurs', 'planting_drugs_guns', 'neglect_of_duty',\n",
        "       'refuse_medical_assistance', 'irrational_aggressive_unstable',\n",
        "       'searching_arresting_minors']\n",
        "\n",
        "\n",
        "    category_map = {}\n",
        "    # Initialize the dictionary with column names\n",
        "    for col in df.columns:\n",
        "        if col in columns_of_interest:\n",
        "            indices = df.index[pd.notnull(df[col])]\n",
        "            indices_true = df.index[df[col] == True]\n",
        "            print(\"Number of labelled documents for {} are {}\".format(col, len(indices)))\n",
        "            print(\"Number of true labels for {} are {}\".format(col, len(indices_true)))\n",
        "            category_map[col] = indices_true\n",
        "\n",
        "    return category_map\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAaia70ax-zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate_words(doc_text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokenized_words = word_tokenize(doc_text)\n",
        "\n",
        "    key_words = ['narrative', 'alleged', 'allege ', 'allogos']\n",
        "\n",
        "    words = [word.lower() for word in tokenized_words if word.lower() not in stop_words and word.isalpha()]\n",
        "    total_words = len(words)\n",
        "    # Perform a sliding window across words near the key words\n",
        "    final_words = []\n",
        "    window_size = 50\n",
        "    for word in key_words:\n",
        "        if word in words:\n",
        "            start_index = words.index(word)+1  # We need the word after the key word, hence +1\n",
        "\n",
        "            # Add next n words to final_words\n",
        "            if start_index + window_size < total_words:\n",
        "                temp = words[start_index: start_index + window_size]\n",
        "            else:\n",
        "                temp = words[start_index: ]\n",
        "\n",
        "            final_words.extend(temp)\n",
        "\n",
        "    if final_words:\n",
        "        # Take only the unique words\n",
        "        final_words = list(set(final_words))\n",
        "        adjacentWords = zip(final_words[0:], final_words[1:], final_words[2:])\n",
        "\n",
        "    return final_words\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NspepuMsyBH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def generate_prob_dict(df):\n",
        "    '''\n",
        "    This function will be called again and again for every category\n",
        "    So we will be generating 11 dictionaries in the end.\n",
        "    :param df:\n",
        "    :return:\n",
        "    '''\n",
        "    word_dict = {}\n",
        "    prob_dict = {}\n",
        "    counter = 0\n",
        "    for text in df['document_text']:\n",
        "        if counter % 5 == 0:\n",
        "            print(\"Processed \", counter, \"records\")\n",
        "        final_words = generate_words(doc_text=text)\n",
        "        if final_words:\n",
        "            adjacentWords = zip(final_words[0: ], final_words[1: ])\n",
        "\n",
        "            for cur, next_word in adjacentWords:\n",
        "                # print(cur, next_word)\n",
        "                # The cartesian product results in tuples having same words. So, ignore those.\n",
        "                if cur != next_word:\n",
        "                    # Format current_word = {next_word_word: number_of_occurrences}\n",
        "\n",
        "                    # If the current word(first) doesn't exist in word dist, then create an entry\n",
        "                    if cur not in word_dict:\n",
        "                        word_dict[cur] = {next_word: 1}\n",
        "\n",
        "                    # If the current word exists but not the adjacent, then create an entry for it\n",
        "                    elif next_word not in word_dict[cur]:\n",
        "                        word_dict[cur][next_word] = 1\n",
        "\n",
        "                    # If word and next_word word exists, increase the count\n",
        "                    else:\n",
        "                        word_dict[cur][next_word] += 1\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "    # Create the probability distribution from the count\n",
        "    for cur, next_words in word_dict.items():\n",
        "        prob_dict[cur] = {}\n",
        "        total_count = sum(next_words.values())\n",
        "        for w in next_words:\n",
        "            prob = next_words[w] / total_count\n",
        "            prob_dict[cur][w] = prob\n",
        "\n",
        "    print(\"Length of probability dictionary\", len(prob_dict))\n",
        "\n",
        "    return prob_dict\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2FLUE8_yDnS",
        "colab_type": "code",
        "outputId": "a4028c81-3cbc-4ff3-8a5a-b9bf47c2953b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Save the document data in a csv file\n",
        "# load_all_tags()\n",
        "# Load the saved data\n",
        "df_doc = pd.read_csv('https://raw.githubusercontent.com/Omkar-Ranadive/CS496-DSS/master/data/document_tags.csv')\n",
        "df_b2015, df_a2015 = clean_data(df_doc)\n",
        "print(\"Before 2015\")\n",
        "cmap_b2015 = divide_by_category(df_b2015)\n",
        "# print(cmap_b2015)\n",
        "print(\"After 2015\")\n",
        "cmap_a2015 = divide_by_category(df_a2015)\n",
        "models_b2015 = []\n",
        "models_a2015 = []\n",
        "model_names = [\n",
        "      'nudity_penetration', 'sexual_harassment_remarks',\n",
        "      'sexual_humiliation_extortion_or_sex_work', 'tasers', 'trespass',\n",
        "      'racial_slurs', 'planting_drugs_guns', 'neglect_of_duty',\n",
        "      'refuse_medical_assistance', 'irrational_aggressive_unstable',\n",
        "      'searching_arresting_minors']\n",
        "# probd = generate_prob_dict(df_b2015.loc[cmap_b2015['nudity_penetration']])\n",
        "# Generate the probability distributions for each class before 2015\n",
        "print(\"--Before 2015--\")\n",
        "for key in model_names:\n",
        "    print(\"Generating distribution for: \", key)\n",
        "    prob_dict = generate_prob_dict(df_b2015.loc[cmap_b2015[key]])\n",
        "    models_b2015.append(prob_dict)\n",
        "\n",
        "print(\"---After 2015---\")\n",
        "# Do the same for data after 2015\n",
        "for key in model_names:\n",
        "    print(\"Generating distribution for: \", key)\n",
        "    prob_dict = generate_prob_dict(df_a2015.loc[cmap_a2015[key]])\n",
        "    models_a2015.append(prob_dict)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before 2015\n",
            "Number of labelled documents for nudity_penetration are 296\n",
            "Number of true labels for nudity_penetration are 3\n",
            "Number of labelled documents for sexual_harassment_remarks are 296\n",
            "Number of true labels for sexual_harassment_remarks are 4\n",
            "Number of labelled documents for sexual_humiliation_extortion_or_sex_work are 296\n",
            "Number of true labels for sexual_humiliation_extortion_or_sex_work are 4\n",
            "Number of labelled documents for tasers are 297\n",
            "Number of true labels for tasers are 114\n",
            "Number of labelled documents for trespass are 296\n",
            "Number of true labels for trespass are 57\n",
            "Number of labelled documents for racial_slurs are 296\n",
            "Number of true labels for racial_slurs are 25\n",
            "Number of labelled documents for planting_drugs_guns are 296\n",
            "Number of true labels for planting_drugs_guns are 24\n",
            "Number of labelled documents for neglect_of_duty are 297\n",
            "Number of true labels for neglect_of_duty are 14\n",
            "Number of labelled documents for refuse_medical_assistance are 296\n",
            "Number of true labels for refuse_medical_assistance are 4\n",
            "Number of labelled documents for irrational_aggressive_unstable are 296\n",
            "Number of true labels for irrational_aggressive_unstable are 2\n",
            "Number of labelled documents for searching_arresting_minors are 296\n",
            "Number of true labels for searching_arresting_minors are 3\n",
            "After 2015\n",
            "Number of labelled documents for nudity_penetration are 43\n",
            "Number of true labels for nudity_penetration are 1\n",
            "Number of labelled documents for sexual_harassment_remarks are 43\n",
            "Number of true labels for sexual_harassment_remarks are 0\n",
            "Number of labelled documents for sexual_humiliation_extortion_or_sex_work are 43\n",
            "Number of true labels for sexual_humiliation_extortion_or_sex_work are 0\n",
            "Number of labelled documents for tasers are 43\n",
            "Number of true labels for tasers are 7\n",
            "Number of labelled documents for trespass are 43\n",
            "Number of true labels for trespass are 2\n",
            "Number of labelled documents for racial_slurs are 43\n",
            "Number of true labels for racial_slurs are 1\n",
            "Number of labelled documents for planting_drugs_guns are 43\n",
            "Number of true labels for planting_drugs_guns are 0\n",
            "Number of labelled documents for neglect_of_duty are 43\n",
            "Number of true labels for neglect_of_duty are 0\n",
            "Number of labelled documents for refuse_medical_assistance are 43\n",
            "Number of true labels for refuse_medical_assistance are 1\n",
            "Number of labelled documents for irrational_aggressive_unstable are 43\n",
            "Number of true labels for irrational_aggressive_unstable are 1\n",
            "Number of labelled documents for searching_arresting_minors are 43\n",
            "Number of true labels for searching_arresting_minors are 0\n",
            "--Before 2015--\n",
            "Generating distribution for:  nudity_penetration\n",
            "Processed  0 records\n",
            "Length of probability dictionary 120\n",
            "Generating distribution for:  sexual_harassment_remarks\n",
            "Processed  0 records\n",
            "Length of probability dictionary 189\n",
            "Generating distribution for:  sexual_humiliation_extortion_or_sex_work\n",
            "Processed  0 records\n",
            "Length of probability dictionary 135\n",
            "Generating distribution for:  tasers\n",
            "Processed  0 records\n",
            "Processed  5 records\n",
            "Processed  10 records\n",
            "Processed  15 records\n",
            "Processed  20 records\n",
            "Processed  25 records\n",
            "Processed  30 records\n",
            "Processed  35 records\n",
            "Processed  40 records\n",
            "Processed  45 records\n",
            "Processed  50 records\n",
            "Processed  55 records\n",
            "Processed  60 records\n",
            "Processed  65 records\n",
            "Processed  70 records\n",
            "Processed  75 records\n",
            "Processed  80 records\n",
            "Processed  85 records\n",
            "Processed  90 records\n",
            "Processed  95 records\n",
            "Processed  100 records\n",
            "Processed  105 records\n",
            "Processed  110 records\n",
            "Length of probability dictionary 2064\n",
            "Generating distribution for:  trespass\n",
            "Processed  0 records\n",
            "Processed  5 records\n",
            "Processed  10 records\n",
            "Processed  15 records\n",
            "Processed  20 records\n",
            "Processed  25 records\n",
            "Processed  30 records\n",
            "Processed  35 records\n",
            "Processed  40 records\n",
            "Processed  45 records\n",
            "Processed  50 records\n",
            "Processed  55 records\n",
            "Length of probability dictionary 1329\n",
            "Generating distribution for:  racial_slurs\n",
            "Processed  0 records\n",
            "Processed  5 records\n",
            "Processed  10 records\n",
            "Processed  15 records\n",
            "Processed  20 records\n",
            "Length of probability dictionary 805\n",
            "Generating distribution for:  planting_drugs_guns\n",
            "Processed  0 records\n",
            "Processed  5 records\n",
            "Processed  10 records\n",
            "Processed  15 records\n",
            "Processed  20 records\n",
            "Length of probability dictionary 706\n",
            "Generating distribution for:  neglect_of_duty\n",
            "Processed  0 records\n",
            "Processed  5 records\n",
            "Processed  10 records\n",
            "Length of probability dictionary 227\n",
            "Generating distribution for:  refuse_medical_assistance\n",
            "Processed  0 records\n",
            "Length of probability dictionary 266\n",
            "Generating distribution for:  irrational_aggressive_unstable\n",
            "Processed  0 records\n",
            "Length of probability dictionary 105\n",
            "Generating distribution for:  searching_arresting_minors\n",
            "Processed  0 records\n",
            "Length of probability dictionary 169\n",
            "---After 2015---\n",
            "Generating distribution for:  nudity_penetration\n",
            "Processed  0 records\n",
            "Length of probability dictionary 66\n",
            "Generating distribution for:  sexual_harassment_remarks\n",
            "Length of probability dictionary 0\n",
            "Generating distribution for:  sexual_humiliation_extortion_or_sex_work\n",
            "Length of probability dictionary 0\n",
            "Generating distribution for:  tasers\n",
            "Processed  0 records\n",
            "Processed  5 records\n",
            "Length of probability dictionary 209\n",
            "Generating distribution for:  trespass\n",
            "Processed  0 records\n",
            "Length of probability dictionary 37\n",
            "Generating distribution for:  racial_slurs\n",
            "Processed  0 records\n",
            "Length of probability dictionary 40\n",
            "Generating distribution for:  planting_drugs_guns\n",
            "Length of probability dictionary 0\n",
            "Generating distribution for:  neglect_of_duty\n",
            "Length of probability dictionary 0\n",
            "Generating distribution for:  refuse_medical_assistance\n",
            "Processed  0 records\n",
            "Length of probability dictionary 38\n",
            "Generating distribution for:  irrational_aggressive_unstable\n",
            "Processed  0 records\n",
            "Length of probability dictionary 79\n",
            "Generating distribution for:  searching_arresting_minors\n",
            "Length of probability dictionary 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4_4piZJzoq_",
        "colab_type": "text"
      },
      "source": [
        "Now, the results of the different probability distributions can be viewed below. Type the numbers as asked in the prompt. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28B3v1O2yTew",
        "colab_type": "code",
        "outputId": "57a278b1-bf64-4e4a-8916-589f77756c06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " while True:\n",
        "        print(\"Select years: \")\n",
        "        print(\"1. Before 2015\")\n",
        "        print(\"2. From 2015\")\n",
        "        print(\"3. Press 3 to terminate\")\n",
        "        year = int(input())\n",
        "        if year == 3: \n",
        "          break \n",
        "        print(\"Select model: \")\n",
        "        for index, name in enumerate(model_names):\n",
        "            print('{}. {}'.format(index, name))\n",
        "\n",
        "        model_num = int(input())\n",
        "        if year == 1:\n",
        "            prob_dist = models_b2015[model_num]\n",
        "        else:\n",
        "            prob_dist = models_a2015[model_num]\n",
        "        \n",
        "        for key, value in prob_dist.items():\n",
        "          print(key, value)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Select years: \n",
            "1. Before 2015\n",
            "2. From 2015\n",
            "3. Press 3 to terminate\n",
            "1\n",
            "Select model: \n",
            "0. nudity_penetration\n",
            "1. sexual_harassment_remarks\n",
            "2. sexual_humiliation_extortion_or_sex_work\n",
            "3. tasers\n",
            "4. trespass\n",
            "5. racial_slurs\n",
            "6. planting_drugs_guns\n",
            "7. neglect_of_duty\n",
            "8. refuse_medical_assistance\n",
            "9. irrational_aggressive_unstable\n",
            "10. searching_arresting_minors\n",
            "0\n",
            "cer {'approximately': 1.0}\n",
            "approximately {'observed': 0.5, 'drugs': 0.5}\n",
            "observed {'page': 1.0}\n",
            "page {'three': 1.0}\n",
            "three {'police': 1.0}\n",
            "police {'ground': 0.5, 'utreras': 0.5}\n",
            "ground {'around': 1.0}\n",
            "around {'standards': 1.0}\n",
            "standards {'summary': 1.0}\n",
            "summary {'custody': 1.0}\n",
            "custody {'iwas': 1.0}\n",
            "iwas {'station': 1.0}\n",
            "station {'rizzi': 1.0}\n",
            "rizzi {'statement': 1.0}\n",
            "statement {'raymond': 1.0}\n",
            "raymond {'report': 1.0}\n",
            "report {'beat': 1.0}\n",
            "beat {'unit': 0.5, 'tified': 0.5}\n",
            "unit {'powder': 1.0}\n",
            "powder {'arrest': 1.0}\n",
            "arrest {'taken': 1.0}\n",
            "taken {'november': 1.0}\n",
            "november {'leached': 1.0}\n",
            "leached {'suspect': 1.0}\n",
            "suspect {'searched': 1.0}\n",
            "searched {'body': 1.0}\n",
            "body {'ofthis': 1.0}\n",
            "ofthis {'removed': 1.0}\n",
            "removed {'foil': 1.0}\n",
            "foil {'dropped': 1.0}\n",
            "dropped {'submitted': 1.0}\n",
            "submitted {'digest': 1.0}\n",
            "digest {'victim': 1.0}\n",
            "victim {'ap': 1.0}\n",
            "ap {'carryin': 1.0}\n",
            "carryin {'stopped': 1.0}\n",
            "stopped {'ohio': 1.0}\n",
            "ohio {'abused': 1.0}\n",
            "abused {'piwnicki': 1.0}\n",
            "piwnicki {'stomped': 1.0}\n",
            "stomped {'operations': 1.0}\n",
            "operations {'ce': 1.0}\n",
            "ce {'special': 1.0}\n",
            "special {'hours': 1.0}\n",
            "hours {'needle': 0.5, 'os': 0.5}\n",
            "needle {'herman': 1.0}\n",
            "herman {'written': 1.0}\n",
            "written {'field': 1.0}\n",
            "field {'feet': 1.0}\n",
            "feet {'cers': 1.0}\n",
            "cers {'vicinity': 1.0}\n",
            "vicinity {'struck': 1.0}\n",
            "struck {'placed': 0.5, 'object': 0.5}\n",
            "placed {'residue': 1.0}\n",
            "residue {'wraps': 1.0}\n",
            "wraps {'allegations': 1.0}\n",
            "allegations {'view': 1.0}\n",
            "view {'states': 1.0}\n",
            "states {'unwrapped': 1.0}\n",
            "unwrapped {'investigation': 1.0}\n",
            "investigation {'alone': 1.0}\n",
            "alone {'officers': 1.0}\n",
            "officers {'ack': 1.0}\n",
            "ack {'clothes': 1.0}\n",
            "clothes {'recovered': 1.0}\n",
            "recovered {'heroin': 1.0}\n",
            "heroin {'transported': 0.5, 'officer': 0.5}\n",
            "transported {'interview': 1.0}\n",
            "interview {'homan': 1.0}\n",
            "homan {'street': 1.0}\n",
            "street {'walking': 1.0}\n",
            "walking {'robert': 1.0}\n",
            "robert {'smith': 1.0}\n",
            "smith {'verbally': 1.0}\n",
            "verbally {'professional': 1.0}\n",
            "professional {'accused': 1.0}\n",
            "accused {'officer': 1.0}\n",
            "officer {'back': 0.5, 'ca': 0.5}\n",
            "back {'joseph': 1.0}\n",
            "joseph {'section': 1.0}\n",
            "section {'stated': 1.0}\n",
            "drugs {'federal': 1.0}\n",
            "federal {'police': 1.0}\n",
            "utreras {'barber': 1.0}\n",
            "barber {'public': 1.0}\n",
            "public {'uniden': 1.0}\n",
            "uniden {'failed': 1.0}\n",
            "failed {'beat': 1.0}\n",
            "tified {'cg': 1.0}\n",
            "cg {'pel': 1.0}\n",
            "pel {'underw': 1.0}\n",
            "underw {'incident': 1.0}\n",
            "incident {'fuod': 1.0}\n",
            "fuod {'return': 1.0}\n",
            "return {'planted': 1.0}\n",
            "planted {'fifty': 1.0}\n",
            "fifty {'inventory': 1.0}\n",
            "inventory {'hours': 1.0}\n",
            "os {'wallet': 1.0}\n",
            "wallet {'lockup': 1.0}\n",
            "lockup {'equipment': 1.0}\n",
            "equipment {'ear': 1.0}\n",
            "ear {'tavernliquor': 1.0}\n",
            "tavernliquor {'struck': 1.0}\n",
            "object {'saiesiresiaurant': 1.0}\n",
            "saiesiresiaurant {'usc': 1.0}\n",
            "usc {'bags': 1.0}\n",
            "bags {'pants': 1.0}\n",
            "pants {'store': 1.0}\n",
            "store {'pulled': 1.0}\n",
            "pulled {'end': 1.0}\n",
            "end {'facigly': 1.0}\n",
            "facigly {'unknown': 1.0}\n",
            "unknown {'codes': 1.0}\n",
            "codes {'heroin': 1.0}\n",
            "ca {'head': 1.0}\n",
            "head {'pit': 1.0}\n",
            "pit {'december': 1.0}\n",
            "december {'took': 1.0}\n",
            "took {'check': 1.0}\n",
            "Select years: \n",
            "1. Before 2015\n",
            "2. From 2015\n",
            "3. Press 3 to terminate\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szaa8Mugzd-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}