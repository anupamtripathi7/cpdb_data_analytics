{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Flagging.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWI4TIT9my49",
        "colab_type": "text"
      },
      "source": [
        "## Flagging officers using Machine Learning: \n",
        "The model was trained using data before 2015 and then tested on data from 2015 onwards. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lwrugVSmx8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQtoqy8BnRex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def clean_data(df):\n",
        "    current_year = 2019\n",
        "\n",
        "    # Replace the null values with mode\n",
        "    col_with_mode = ['gender', 'birth_year', 'rank', 'race', 'appointed_date']\n",
        "    for col in col_with_mode:\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "    # Get only the year of the appointed date\n",
        "    df['appointed_date'] = pd.to_datetime(df['appointed_date'])\n",
        "    df['appointed_date'] = pd.DatetimeIndex(df['appointed_date']).year\n",
        "\n",
        "    # Categorize the data\n",
        "    data_to_encode = ['race', 'rank']\n",
        "    df[data_to_encode] = df[data_to_encode].apply(lambda col: encoder.fit_transform(col))\n",
        "\n",
        "    # Convert birth year to actual age\n",
        "    df['birth_year'] = current_year - df['birth_year']\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xpQ2Z6mrnRu",
        "colab_type": "text"
      },
      "source": [
        "We choose five features for each officer: Race, Rank, Age (The column name is birth year but the current year 2019 has been substracted from it giving us the age), Appointed Date (only the year), Number of awards. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFVJ0bR2nW6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_features(df, awards, normalize_data=True):\n",
        "    feature_set = ['race', 'rank', 'birth_year', 'appointed_date', 'awards']\n",
        "\n",
        "    # Generate the awards column\n",
        "    new_col = []\n",
        "\n",
        "    for off_id in df['id']:\n",
        "        if off_id in awards:\n",
        "            new_col.append(awards[off_id])\n",
        "        else:\n",
        "            new_col.append(0)\n",
        "\n",
        "    df['awards'] = new_col\n",
        "\n",
        "    data = df[feature_set].values\n",
        "    if normalize_data:\n",
        "        data = normalize(data)\n",
        "\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpu5CqZZnZnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def calculate_accuracy(predicted, true_labels, threshold=0.7):\n",
        "    '''\n",
        "\n",
        "    :param predicted: Predicted values are in terms of probabilities\n",
        "    :param true_labels: Actual labels 0, 1\n",
        "    :param threshold:  The threshold to use (0.5, 0.6, 0.7)\n",
        "    :return:\n",
        "    '''\n",
        "\n",
        "    predicted = np.where(predicted >= threshold, 1, 0)\n",
        "\n",
        "    accuracy = ((true_labels == predicted).sum())/true_labels.shape[0]\n",
        "\n",
        "    return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGOP-APlncsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Load the data\n",
        "  df_bcb2015 = pd.read_csv('https://raw.githubusercontent.com/Omkar-Ranadive/CS496-DSS/master/data/badCopsB2015.csv')\n",
        "  df_bca2015 = pd.read_csv('https://raw.githubusercontent.com/Omkar-Ranadive/CS496-DSS/master/data/badCopsA2015.csv')\n",
        "  df_gcb2015 = pd.read_csv('https://raw.githubusercontent.com/Omkar-Ranadive/CS496-DSS/master/data/goodCopsB2015.csv')\n",
        "  df_gca2015 = pd.read_csv('https://raw.githubusercontent.com/Omkar-Ranadive/CS496-DSS/master/data/goodCopsA2015.csv')\n",
        "  df_awards = pd.read_csv('https://raw.githubusercontent.com/Omkar-Ranadive/CS496-DSS/master/data/offAwards.csv')\n",
        "  # Convert awards to a dictionary\n",
        "  awards_dict = dict(zip(df_awards['officer_id'], df_awards['count']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQfPPdbipYjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Global initializations\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Clean the data\n",
        "df_bcb2015 = clean_data(df_bcb2015)\n",
        "df_bca2015 = clean_data(df_bca2015)\n",
        "df_gcb2015 = clean_data(df_gcb2015)\n",
        "df_gca2015 = clean_data(df_gca2015)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFTZDNzKqEFQ",
        "colab_type": "code",
        "outputId": "d5b806aa-6b70-4bd1-a064-a2379080d709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# Create the features for training and testing\n",
        "# Labels Bad Cops = 1, Good Cops = 0\n",
        "X1_train = create_features(df_bcb2015, awards_dict)\n",
        "Y1_train = np.ones((X1_train.shape[0], 1))\n",
        "X2_train = create_features(df_gcb2015, awards_dict)\n",
        "Y2_train = np.zeros((X2_train.shape[0], 1))\n",
        "\n",
        "# Join the training sets\n",
        "print(X1_train.shape, X2_train.shape)\n",
        "print(Y1_train.shape, Y2_train.shape)\n",
        "X_train = np.vstack((X1_train, X2_train))\n",
        "Y_train = np.vstack((Y1_train, Y2_train))\n",
        "Y_train = Y_train.ravel()  # Flatten the array from (n, 1) to (n, )\n",
        "print(X_train.shape, Y_train.shape)\n",
        "\n",
        "# Do the same thing for test set\n",
        "X1_test = create_features(df_bca2015, awards_dict)\n",
        "Y1_test = np.ones((X1_test.shape[0], 1))\n",
        "X2_test = create_features(df_gca2015, awards_dict)\n",
        "Y2_test = np.zeros((X2_test.shape[0], 1))\n",
        "\n",
        "# Join the testing sets\n",
        "print(X1_test.shape, X2_test.shape)\n",
        "print(Y1_test.shape, Y2_test.shape)\n",
        "X_test = np.vstack((X1_test, X2_test))\n",
        "Y_test = np.vstack((Y1_test, Y2_test))\n",
        "Y_test = Y_test.ravel()  # Flatten the array from (n, 1) to (n, )\n",
        "print(X_test.shape, Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(22330, 5) (11341, 5)\n",
            "(22330, 1) (11341, 1)\n",
            "(33671, 5) (33671,)\n",
            "(4396, 5) (29275, 5)\n",
            "(4396, 1) (29275, 1)\n",
            "(33671, 5) (33671,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7RsdlzXqUEa",
        "colab_type": "code",
        "outputId": "97d8bfa6-27a5-4a7d-aa6a-e26bab6eff5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        " \n",
        "# Create Logistic Classifier for training and testing\n",
        "clf_log = LogisticRegression(random_state=0)\n",
        "clf_log.fit(X_train, Y_train)\n",
        "probabilities = clf_log.predict_proba(X_train)\n",
        "# print(type(probabilities), probabilities)\n",
        "# acc_train = calculate_accuracy(probabilities, Y_train, threshold=0.7)\n",
        "# print(acc_train)\n",
        "acc_train = clf_log.score(X_train, Y_train)\n",
        "print(\"Train accuracy is: \", acc_train)\n",
        "acc_test = clf_log.score(X_test, Y_test)\n",
        "print(\"Test accuracy is: \", acc_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is:  0.666240978883906\n",
            "Test accuracy is:  0.13278488907368358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx75jUdYqtAx",
        "colab_type": "text"
      },
      "source": [
        "As we can see, the test accuracy is significantly lower than the training accuracy. We would like to emphasize that this **is a good thing in our case.**\n",
        "The low testing accuracy shows that the test distribution is different from the training distribution. That is, the data before the launch of CPDB and the data after the launch of CPDB has signficant differences in its trends. Hence, our hypothesis that there has been significant changes in the trends after CPDB got released is validated. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK1U9sqarV-A",
        "colab_type": "text"
      },
      "source": [
        "The same data is now trained using SVM. This gives similar results to Logistic Regression. N**ote: The following cell should take a couple of minutes to run.** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjW8HIXpqYMt",
        "colab_type": "code",
        "outputId": "b6d6e077-5f3d-409d-e794-a80372c2409a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "clf_svm = SVC(gamma='auto')\n",
        "clf_svm.fit(X_train, Y_train)\n",
        "print(\"Train accuracy is: \", clf_svm.score(X_train, Y_train))\n",
        "print(\"Test accuracy is: \", clf_svm.score(X_test, Y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy is:  0.6631819666775564\n",
            "Test accuracy is:  0.13055745300109886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHacWbjIqbOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}